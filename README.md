Download Link: https://assignmentchef.com/product/solved-stat435-homework5
<br>
<strong>Problem 1:</strong> Consider a classification problem with a binary class label <em>Y </em>and a single continuous feature <em>X </em>that takes values in (−4<em>,</em>−2) ∪ (2<em>,</em>4). Suppose (<em>X,Y </em>) is generated by choosing <em>Y </em>at random with <em>P</em>(<em>Y </em>= 1) = <em>P</em>(<em>Y </em>= 2) = 1<em>/</em>2, and then drawing <em>X </em>conditional on <em>Y </em>according to uniform distributions. Specifically, assume that the class-conditional densities for <em>X </em>are

)      and

In the below we consider 0-1 loss, that is, the risk of a classifier is the probability of an error.

<ul>

 <li> What is the marginal distribution of <em>X</em>? What is the conditional distribution of <em>Y </em>given <em>X</em>?</li>

 <li> What is the Bayes rule <em>f<sub>B</sub></em>(<em>x</em>) and its risk <em>P</em>(<em>Y </em>6= <em>f<sub>B</sub></em>(<em>x</em>)?</li>

</ul>

Explain!

<ul>

 <li>Let <em>f</em><sup>ˆ</sup><sub>1</sub>(<em>x</em>;<em>S</em>) be the 1-nearest neighbor classifier based on a training sample <em>S </em>= {(<em>x</em><sub>1</sub><em>,y</em><sub>1</sub>)<em>,…,</em>(<em>x<sub>n</sub>,y<sub>n</sub></em>)} of i.i.d observations of (<em>X,Y </em>). What is the risk Pr(<em>Y </em>6= <em>f</em><sup>ˆ</sup><sub>1</sub>(<em>X</em>;<em>S</em>)? Explain. (Here, the risk is computed by integrating over training data and a new independent pair (<em>X,Y </em>)).</li>

 <li> Under the same scenario calculate the risk of the 3-nearest neighbor classifier.</li>

 <li> Which method, 1-nearest neighbor or 3-nearest neighbor, has smaller risk in this problem?</li>

</ul>

<ol start="2">

 <li><strong> ISLR Section 8.4 Problem 3 </strong></li>

 <li><strong> ISLR Section 8.4 Problem 9 (a) …(g)</strong></li>

</ol>

1